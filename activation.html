<!DOCTYPE html>
<html lang="en" data-theme="light">

<head>
    <title>Mahitha Chodavarapu | Learnings</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="application-name" content="Mahitha Chodavarapu | Home" />
    <meta name="apple-mobile-web-app-title" content="Mahitha Chodavarapu | Home" />
    
    <!--For Dark/Light mode Toggle-->
    <script src="https://code.iconify.design/1/1.0.4/iconify.min.js"></script>
    <!--Import Google Icon Font-->
    <link href="assets/img/favicon.png" rel="icon">
    <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Template Main CSS File -->
    <link href="assets/css/style1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" />
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300&display=swap" rel="stylesheet">
    <script src='https://kit.fontawesome.com/a076d05399.js'></script>
    <link rel="stylesheet" href="assets/css/preloader.css">
    <link rel="stylesheet" href="assets/css/home.css"> 
    <link rel="stylesheet" href="assets/css/style.css"> 
    <!-- Favicon -->
    <link id='favicon' rel="shortcut icon" href="assets/images/favicon.png" type="image/x-png"> 
    <!-- Font awesome icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script defer src="assets\js\dynamicTitle.js"></script>
</head>

<body>
<!-- loader -->
<br>
<br>
      
<section id="about" class="about">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          

          <h2><strong>What is an Activation Function?</strong></h2>
<p>The input layer of the neural network receives data for training which comes in different formats like images, audio, or texts. From the dataset, input features with weights and biases are used to calculate the linear function. Then, this resultant from the linear function is used by the activation function as input and calculated activations are further fed as input to the next layer.</p>
<div class="medium-insert-images">
<figure><img class="aligncenter" src="https://editor.analyticsvidhya.com/uploads/49097ACT.gif" alt="Activation function" width="466" height="202" /></figure>
</div>
<p>&nbsp;</p>
<h2 align="center">&nbsp;</h2>
<p>Basically, three important steps took place in a single iteration of deep neural architectures: Forward Propagation, Backward Propagation, and Gradient Descent(Optimization).</p>
<p>1.&nbsp;Forward Propagation: In this step input data is fed in the forward direction through each layer of the neural network. The linear calculation takes place in this step and the activation function is applied.</p>
<p>2.&nbsp;Back Propagation: This step helps in calculating all the derivatives which will be further used for Optimization or updating the parameters.</p>
<p>3.&nbsp;Optimization: This step helps in the convergence of the loss function by continuously updating the parameters in each iteration. Some optimization algorithms are as follows: Gradient Descent, Momentum, Adam, AdaGrad, RMSProp, etc.</p>
<p>&nbsp;</p>
<h2><strong>Let us discuss Activation Functions</strong></h2>
<h3><strong>1. Sigmoid Function</strong></h3>
<p>&middot;&nbsp;The biggest advantage that it has over other steps and linear functions is its non-linearity. The function ranges from 0 to 1 having an S shape.&nbsp;Also known by the name of the logistic or squashing function in some literature. The sigmoid function is used in output layers of the DNN and is used for probability-based output.</p>
<p>Its major drawbacks are sharp damp gradients during backpropagation, gradient saturation, slow convergence, and non-zero centered output thereby causing the gradient updates to propagate in different directions</p>
<div class="medium-insert-images">
<figure><img class="aligncenter" src="https://editor.analyticsvidhya.com/uploads/15533SIG.jpg" alt="Sigmoid Activation Function" width="400" height="300" /></figure>
</div>
<p class=""><strong>f(x)=1/(1+e^(-x)</strong></p>
<p>Other Variants:</p>
<p>I.&nbsp;Hard Sigmoid Function</p>
<p>II.&nbsp;Sigmoid Weigted Linear Units(SiLU)</p>
<p>&nbsp;</p>
<h3><strong>2. TanH Function</strong></h3>
<p>&middot;&nbsp;The hyperbolic tangent function is a zero-centered function and its range lies between -1 to 1</p>
<p>&middot;&nbsp;As this function is zero centered, this makes it easier to model inputs that have strongly negative, neutral, and strongly positive values</p>
<p>&middot;&nbsp;It is advised to use tanh function instead of sigmoid function if your output is other than 0 and 1</p>
<p>&middot;&nbsp;The tanh functions have been used mostly in RNN for natural language processing and speech recognition tasks</p>
<div class="medium-insert-images">
<figure><img class="aligncenter" src="https://editor.analyticsvidhya.com/uploads/42116TAN.gif" alt="TanH Function" width="360" height="233" /></figure>
</div>
<p class=""><strong>f(x)=(e^x-e^(-x))/(e^x+e^(-x)<br /></strong></p>
<p>&nbsp;</p>
<h3><strong>3. Rectified Linear Unit(ReLU)</strong></h3>
<p><strong>&middot;&nbsp;</strong>ReLU has been the most widely used activation function for DL applications with state-of-the-art results</p>
<p><strong>&middot;&nbsp;</strong>It provides the upper hand in performance and generalization compared to the Sigmoid and Tanh activation functions</p>
<p><strong>&middot;&nbsp;</strong>Along with the overall speed of computation enhanced, ReLU provides faster computation since it does not compute exponentials and divisions</p>
<p><strong>&middot;&nbsp;</strong>It easily overfits compared to the sigmoid function and is one of the main limitations. Some techniques like dropout are used to reduce the overfitting</p>
<div class="medium-insert-images">
<figure><img class="aligncenter" src="https://editor.analyticsvidhya.com/uploads/39759RELU.jpg" alt="Rectified Linear Unit(ReLU) " width="311" height="210" /></figure>
</div>
<p class=""><strong>f(x)=x if x&ge;0 and 0 if x&lt;0</strong></p>
<h2>&nbsp;</h2>
<h2><em>f</em><em>x</em><em>=x if x&ge;0 and 0<br />if x&lt;0</em></h2>
<p>&nbsp;</p>
<p>Other variants:</p>
<p>I.&nbsp;Leaky ReLU(LReLU)</p>
<p>II.&nbsp;Parametric ReLU(PReLU)</p>
<p>III.&nbsp;Randomised Leaky ReLU(RReLU)</p>
<p>IV.&nbsp;S-Shaped ReLU(SReLU)</p>
          </div>
 </div>
    </section><!-- End About Section -->

    <!-- Dynamic footer section -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
        integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167979537-2"></script>
    <!-- Fetching our Google Tag Manager -->

    <!--JavaScript at end of body for optimized loading-->
    <script src="assets/js/app.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/particles.js/2.0.0/particles.min.js'></script>
    <script src="assets/js/particle.js"></script>

</body>

</html>